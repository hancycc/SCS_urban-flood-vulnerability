from xgboost import XGBClassifier
import numpy as np
import pandas as pd
from osgeo import gdal
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, cohen_kappa_score, roc_auc_score
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import joblib


def replace_invalid_values(array, invalid_value):
    array = array.astype(float)
    array[array <= invalid_value] = np.nan
    return array


def replace_invalid_values2(array, invalid_value):
    array = array.astype(float)
    array[array == invalid_value] = np.nan
    return array


def dataset_unit(year):
    # 读取遥感数据
    AI_path = r'D:\@cc数据\内涝因子_Nor\AI\ai2020_nor.tif'
    Elevation_path = r'D:\@cc数据\内涝因子_Nor\Elevation\Elevation_nor.tif'
    FVC_path = r'D:\@cc数据\内涝因子_Nor\FVC\fvc2020_nor.tif'
    ISP_path = r'D:\@cc数据\内涝因子_Nor\ISP\ISP2020_nor.tif'
    lithology_path = r'D:\@cc数据\内涝因子_Nor\lithology\lithology_nor.tif'
    LULC_path = r'D:\@cc数据\内涝因子_Nor\LULC\lucc2020_nor.tif'
    PRE_path = r'D:\@cc数据\内涝因子_Nor\Preciptation\pre2020_nor.tif'
    SHDI_path = r'D:\@cc数据\内涝因子_Nor\SHDI\shdi2020_nor.tif'
    Slope_path = r'D:\@cc数据\内涝因子_Nor\slope\slope_nor.tif'
    SWR_path = r'D:\@cc数据\内涝因子_Nor\SWR\SWR2020_nor.tif'
    point_path = r'C:\Users\A\Desktop\3.内涝点合成_重新整理\pointn.tif'

    AI = replace_invalid_values(gdal.Open(AI_path).ReadAsArray().flatten(), -340000000)
    Elevation = replace_invalid_values(gdal.Open(Elevation_path).ReadAsArray().flatten(), -34000000)
    FVC = replace_invalid_values(gdal.Open(FVC_path).ReadAsArray().flatten(), -34000000)
    ISP = replace_invalid_values(gdal.Open(ISP_path).ReadAsArray().flatten(), -3400000000)
    lithology = replace_invalid_values(gdal.Open(lithology_path).ReadAsArray().flatten(), -3400000000)
    LULC = replace_invalid_values(gdal.Open(LULC_path).ReadAsArray().flatten(), -3400000000)
    SHDI = replace_invalid_values(gdal.Open(SHDI_path).ReadAsArray().flatten(), -34000000000)
    PRE = replace_invalid_values(gdal.Open(PRE_path).ReadAsArray().flatten(), -34000000000)
    Slope = replace_invalid_values(gdal.Open(Slope_path).ReadAsArray().flatten(), -3400000000)
    SWR = replace_invalid_values(gdal.Open(SWR_path).ReadAsArray().flatten(), -3400000000)
    point = replace_invalid_values2(gdal.Open(point_path).ReadAsArray().flatten(), -128)

    # 创建数据框
    data = pd.DataFrame({
        'AI': AI,
        'Elevation': Elevation,
        'FVC': FVC,
        'ISP': ISP,
        'Lithology': lithology,
        'LULC': LULC,
        'SHDI': SHDI,
        'PRE': PRE,
        'Slope': Slope,
        'SWR': SWR,
        'Target': point,
    })

    # 删除包含缺失值的行
    data = data.dropna()
    return data


def train_XGboost(data, n_iterations=1000):
    result_summary = []

    # 提取正样本和负样本
    positive_samples = data[data['Target'] == 1]
    negative_samples = data[data['Target'] == -1]

    # 随机选择与正样本数目相同的负样本
    negative_samples = negative_samples.sample(n=len(positive_samples))

    # 将正负样本组合成一个新的数据集
    data = pd.concat([positive_samples, negative_samples])

    for i in range(n_iterations):
        # 定义模型名
        model_name = f'model_{i}'
        # 随机打乱数据
        data_shuffled = shuffle(data)
        # 划分训练集和测试集
        X_train, X_test, y_train, y_test = train_test_split(data_shuffled.iloc[:, :-1], data_shuffled['Target'],
                                                            test_size=0.3)

        # 训练XGBoost模型
        model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=500, colsample_bytree=0.33)
        # 将 -1 映射到 0，1 映射到 1
        y_train_mapped = (y_train + 1) / 2
        y_test_mapped = (y_test + 1) / 2
        # 使用映射后的标签训练模型
        model.fit(X_train, y_train_mapped)
        # 预测训练集和测试集
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # 评估模型性能
        oa_train = accuracy_score(y_train_mapped, y_train_pred)
        kappa_train = cohen_kappa_score(y_train_mapped, y_train_pred)
        auc_train = roc_auc_score(y_train_mapped, model.predict_proba(X_train)[:, 1])

        oa_test = accuracy_score(y_test_mapped, y_test_pred)
        kappa_test = cohen_kappa_score(y_test_mapped, y_test_pred)
        auc_test = roc_auc_score(y_test_mapped, model.predict_proba(X_test)[:, 1])

        result_summary.append([i, oa_train, kappa_train, auc_train, oa_test, kappa_test, auc_test])

        if kappa_test > 0.85 and auc_test > 0.95:
            # 保存模型
            save_path = f'C:\\Users\\A\\Desktop\\公管_返修\\数据\\XGboost\\{model_name}.joblib'  # 根据实际情况修改路径
            joblib.dump(model, save_path)
            print(f'Model {model_name} saved to {save_path}')
        else:
            print(f'Model {model_name} not saved.')

    # 将结果保存到 CSV 文件
    result_df = pd.DataFrame(result_summary,
                             columns=['Iteration', 'OA_train', 'Kappa_train', 'AUC_train', 'OA_test', 'Kappa_test',
                                      'AUC_test'])
    result_df.to_csv(r'C:\Users\A\Desktop\公管_返修\数据\XGboost\\accuracy_XG.csv', index=False)


if __name__ == "__main__":
    # 读取 2020 年的数据
    data = dataset_unit(2020)
    # 训练随机森林模型并保存
    train_XGboost(data)
