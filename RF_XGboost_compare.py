import time
import numpy as np
import pandas as pd
from osgeo import gdal
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import cohen_kappa_score, roc_auc_score
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split


def replace_invalid_values(array, invalid_value):
    array = array.astype(float)
    array[array <= invalid_value] = np.nan
    return array


def dataset_unit(year):
    AI_path = r'D:\@cc数据\内涝因子_Nor\AI\ai2020_nor.tif'
    Elevation_path = r'D:\@cc数据\内涝因子_Nor\Elevation\Elevation_nor.tif'
    FVC_path = r'D:\@cc数据\内涝因子_Nor\FVC\fvc2020_nor.tif'
    ISP_path = r'D:\@cc数据\内涝因子_Nor\ISP\ISP2020_nor.tif'
    lithology_path = r'D:\@cc数据\内涝因子_Nor\lithology\lithology_nor.tif'
    LULC_path = r'D:\@cc数据\内涝因子_Nor\LULC\lucc2020_nor.tif'
    PRE_path = r'D:\@cc数据\内涝因子_Nor\Preciptation\pre2020_nor.tif'
    SHDI_path = r'D:\@cc数据\内涝因子_Nor\SHDI\shdi2020_nor.tif'
    Slope_path = r'D:\@cc数据\内涝因子_Nor\slope\slope_nor.tif'
    SWR_path = r'D:\@cc数据\内涝因子_Nor\SWR\SWR2020_nor.tif'
    point_path = r'C:\Users\A\Desktop\3.内涝点合成_重新整理\pointn.tif'

    AI = gdal.Open(AI_path).ReadAsArray()
    rows, cols = AI.shape

    AI = replace_invalid_values(AI.flatten(), -340000000)
    Elevation = replace_invalid_values(gdal.Open(Elevation_path).ReadAsArray().flatten(), -34000000)
    FVC = replace_invalid_values(gdal.Open(FVC_path).ReadAsArray().flatten(), -34000000)
    ISP = replace_invalid_values(gdal.Open(ISP_path).ReadAsArray().flatten(), -3400000000)
    lithology = replace_invalid_values(gdal.Open(lithology_path).ReadAsArray().flatten(), -3400000000)
    LULC = replace_invalid_values(gdal.Open(LULC_path).ReadAsArray().flatten(), -3400000000)
    SHDI = replace_invalid_values(gdal.Open(SHDI_path).ReadAsArray().flatten(), -34000000000)
    PRE = replace_invalid_values(gdal.Open(PRE_path).ReadAsArray().flatten(), -34000000000)
    Slope = replace_invalid_values(gdal.Open(Slope_path).ReadAsArray().flatten(), -3400000000)
    SWR = replace_invalid_values(gdal.Open(SWR_path).ReadAsArray().flatten(), -3400000000)
    point = replace_invalid_values(gdal.Open(point_path).ReadAsArray().flatten(), -128)
    data = pd.DataFrame({
        'AI': AI,
        'Elevation': Elevation,
        'FVC': FVC,
        'ISP': ISP,
        'Lithology': lithology,
        'LULC': LULC,
        'SHDI': SHDI,
        'PRE': PRE,
        'Slope': Slope,
        'SWR': SWR,
        'Target': point,  # 将 point 转为一维数组
    })

    data = data.dropna()
    return data


def compare_models(data, n_iterations=1000):
    results = []

    for i in range(n_iterations):
        # 提取正样本和负样本
        positive_samples = data[data['Target'] == 1]
        negative_samples = data[data['Target'] == -1]

        # 随机选择与正样本数目相同的负样本
        negative_samples = negative_samples.sample(n=len(positive_samples))

        # 将正负样本组合成一个新的数据集
        data_combined = pd.concat([positive_samples, negative_samples])

        # 随机打乱数据
        data_shuffled = shuffle(data_combined)

        # 划分训练集和测试集
        X_train, X_test, y_train, y_test = train_test_split(data_shuffled.iloc[:, :-1], data_shuffled['Target'],
                                                            test_size=0.3)

        # 定义模型名
        model_name = f'model_{i}'

        # 训练 XGBoost 模型
        xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=500,
                                  colsample_bytree=0.33)
        y_train_mapped = (y_train + 1) / 2
        y_test_mapped = (y_test + 1) / 2

        start_time_xgb = time.time()
        xgb_model.fit(X_train, y_train_mapped)
        end_time_xgb = time.time()

        y_train_pred_xgb = xgb_model.predict(X_train)
        y_test_pred_xgb = xgb_model.predict(X_test)

        kappa_train_xgb = cohen_kappa_score(y_train_mapped, y_train_pred_xgb)
        auc_train_xgb = roc_auc_score(y_train_mapped, xgb_model.predict_proba(X_train)[:, 1])

        kappa_test_xgb = cohen_kappa_score(y_test_mapped, y_test_pred_xgb)
        auc_test_xgb = roc_auc_score(y_test_mapped, xgb_model.predict_proba(X_test)[:, 1])
        print(i)
        # 训练随机森林模型
        rf_model = RandomForestClassifier(n_estimators=500, max_features=3)

        start_time_rf = time.time()
        rf_model.fit(X_train, y_train)
        end_time_rf = time.time()

        y_train_pred_rf = rf_model.predict(X_train)
        y_test_pred_rf = rf_model.predict(X_test)

        kappa_train_rf = cohen_kappa_score(y_train, y_train_pred_rf)
        auc_train_rf = roc_auc_score(y_train, rf_model.predict_proba(X_train)[:, 1])

        kappa_test_rf = cohen_kappa_score(y_test, y_test_pred_rf)
        auc_test_rf = roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])
        print(i)
        results.append({
            'Iteration': i + 1,
            'Model_Name': model_name,
            'Kappa_train_xgb': kappa_train_xgb,
            'AUC_train_xgb': auc_train_xgb,
            'Kappa_test_xgb': kappa_test_xgb,
            'AUC_test_xgb': auc_test_xgb,
            'Training_Time_xgb': end_time_xgb - start_time_xgb,
            'Kappa_train_rf': kappa_train_rf,
            'AUC_train_rf': auc_train_rf,
            'Kappa_test_rf': kappa_test_rf,
            'AUC_test_rf': auc_test_rf,
            'Training_Time_rf': end_time_rf - start_time_rf,
        })

    # 输出结果到CSV文件
    results_df = pd.DataFrame(results)
    results_df.to_csv('model_comparison_results.csv', index=False)



if __name__ == "__main__":
    data = dataset_unit(2020)
    compare_models(data, n_iterations=1000)
